{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/artificial-intelligence.png\" style=\"height:50px;display:inline\"> EE 046202 - Technion - Unsupervised Learning & Data Analysis\n",
    "---\n",
    "\n",
    "## Computer Assignment 2 - Variational Autoencoders & Clustering\n",
    "---\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* Variational Autoencoders (VAEs)\n",
    "    * $\\beta$-VAE\n",
    "    * Conditional VAE (CVAE)\n",
    "    * BONUS: Generating Pokemons\n",
    "* Clustering with GNNs\n",
    "\n",
    "#### Use as many cells as you need\n",
    "#### אפשר גם לכתוב בעברית, אבל עדיף באנגלית\n",
    "\n",
    "* Code Tasks are denoted with: <img src=\"https://img.icons8.com/color/48/null/code.png\">\n",
    "* Questions (which you need to answer in a Markdown cell) are denoted with: <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/null/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/null/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: **100** (even with the bonus, the grade will not be above 100).\n",
    "    * Example: if you got 5 points bonus, but you were right in all sections, your grade will still be 100 (and not 105).\n",
    "    * Example: if you got 5 points bonus, and 6 points were deducted for wrong answers, your grade will be 99.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **BONUS**:\n",
    "    * 5 points - completing the Pokemon task with *fully-connected* layers (black & white version)\n",
    "    * 10 points - completing the Pokemon task with *convolutional* layers (RGB version)\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * Submit a `.zip` file with the name `ee046202_wet2_id1_id2.zip` with content:\n",
    "        * `ee046202_wet2_id1_id2.ipynb` - the code tasks\n",
    "        * `ee046202_wet2_id1_id2.pdf` - answers to questions. If you solved everything in the notebook, print the notebook as pdf.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/null/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/null/info.png\" style=\"height:50px;display:inline\"> Tip\n",
    "---\n",
    "If you find it more convenient, you can copy the section to a new cell, and answer the question or rite the code just right below it. For example:\n",
    "\n",
    "#### Question 0\n",
    "1. What is the best course in the Technion?\n",
    "2. Why does no one pick Bulbasaur as first pokemon?\n",
    "3. Why is there no superhero named Catman?\n",
    "\n",
    "#### Answers - Q0\n",
    "\n",
    "#### Q0 - Section 1\n",
    "* Q: What is the best course in the Technion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"ANAM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q0 - Section 2\n",
    "* Q: Why does no one pick Bulbasaur as first pokemon?\n",
    "\n",
    "It is really a riddle....\n",
    "\n",
    "#### Q0 - Section 3\n",
    "* Q: Why is there no superhero named Catman?\n",
    "\n",
    "I got nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/null/grand-master-key.png\" style=\"height:50px;display:inline\"> Part 1 - Variational Autoencoders - Prologue\n",
    "---\n",
    "In this section of the exercise we will analyze the VAE and introduce an enhacement called $\\beta$-VAE and also a variation of VAE that allows us some control over the latent space using conditional probability - Conditional Variational Autoencoder.\n",
    "\n",
    "For this part, you are provided the original implementation of the VAE from the tutorial. You will have to modify it throughout the tasks (yes, you can copy-paste from the original implementation).\n",
    "* **Note** - for better results you should tune the model!\n",
    "    * You can add layers / hidden units / different activations (ReLU, TanH, LeakyReLU, Sigmoid...)\n",
    "    * You can choose a different optimizer than Adam (SGD, RMSProp...), tune the learning rate...\n",
    "    * You can change the reconstruction loss (BCE, MSE, L1...)\n",
    "    * Other hyper-parameters like the batch-size, number of epochs and etc...\n",
    "\n",
    "We recommend running this part on Google Colab or on a GPU (if you have an access to one). Note that running on a GPU will lead to about x2 speedup in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 1\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original implementation from the tutorial - leave untouched (for your own sake), copy-paste what you need to another cell\n",
    "\n",
    "# reparametrization trick\n",
    "def reparameterize(mu, logvar, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    This function applies the reparameterization trick:\n",
    "    z = mu(X) + sigma(X)^0.5 * epsilon, where epsilon ~ N(0,I)\n",
    "    :param mu: mean of x\n",
    "    :param logvar: log variance of x\n",
    "    :param device: device to perform calculations on\n",
    "    :return z: the sampled latent variable\n",
    "    \"\"\"\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std).to(device)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "# encoder - Q(z|X)\n",
    "class VaeEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "       This class builds the encoder for the VAE\n",
    "       :param x_dim: input dimensions\n",
    "       :param hidden_size: hidden layer size\n",
    "       :param z_dim: latent dimensions\n",
    "       :param device: cpu or gpu\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim=28*28, hidden_size=256, z_dim=10, device=torch.device(\"cpu\")):\n",
    "        super(VaeEncoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_dim = z_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.features = nn.Sequential(nn.Linear(x_dim, self.hidden_size),\n",
    "                                      nn.ReLU())\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.z_dim, bias=True)  # fully-connected to output mu\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.z_dim, bias=True)  # fully-connected to output logvar\n",
    "\n",
    "\n",
    "    def bottleneck(self, h):\n",
    "        \"\"\"\n",
    "        This function takes features from the encoder and outputs mu, log-var and a latent space vector z\n",
    "        :param h: features from the encoder\n",
    "        :return: z, mu, log-variance\n",
    "        \"\"\"\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        # use the reparametrization trick as torch.normal(mu, logvar.exp()) is not differentiable\n",
    "        z = reparameterize(mu, logvar, device=self.device)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        z, mu, logvar = VaeEncoder(X)\n",
    "        \"\"\"\n",
    "        h = self.features(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    \n",
    "class VaeDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "       This class builds the decoder for the VAE\n",
    "       :param x_dim: input dimensions\n",
    "       :param hidden_size: hidden layer size\n",
    "       :param z_dim: latent dimensions\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim=28*28, hidden_size=256, z_dim=10):\n",
    "        super(VaeDecoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(self.z_dim, self.hidden_size),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.hidden_size, self.x_dim),\n",
    "                                     nn.Sigmoid())\n",
    "        # why we use sigmoid? becaue the pixel values of images are in [0,1] and sigmoid(x) does just that!\n",
    "        # if you don't work with images, you don't have to use that.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        x_reconstruction = VaeDecoder(z)\n",
    "        \"\"\"\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Vae(torch.nn.Module):\n",
    "    def __init__(self, x_dim=28*28, z_dim=10, hidden_size=256, device=torch.device(\"cpu\")):\n",
    "        super(Vae, self).__init__()\n",
    "        self.device = device\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.encoder = VaeEncoder(x_dim, hidden_size, z_dim=z_dim, device=device)\n",
    "        self.decoder = VaeDecoder(x_dim, hidden_size, z_dim=z_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def sample(self, num_samples=1):\n",
    "        \"\"\"\n",
    "        This functions generates new data by sampling random variables and decoding them.\n",
    "        Vae.sample() actually generatess new data!\n",
    "        Sample z ~ N(0,1)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.z_dim).to(self.device)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the function called when doing the forward pass:\n",
    "        return x_recon, mu, logvar, z = Vae(X)\n",
    "        \"\"\"\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar, z\n",
    "    \n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, loss_type='bce'):\n",
    "    \"\"\"\n",
    "    This function calculates the loss of the VAE.\n",
    "    loss = reconstruction_loss - 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param recon_x: the reconstruction from the decoder\n",
    "    :param x: the original input\n",
    "    :param mu: the mean given X, from the encoder\n",
    "    :param logvar: the log-variance given X, from the encoder\n",
    "    :param loss_type: type of loss function - 'mse', 'l1', 'bce'\n",
    "    :return: VAE loss\n",
    "    \"\"\"\n",
    "    if loss_type == 'mse':\n",
    "        recon_error = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    elif loss_type == 'l1':\n",
    "        recon_error = F.l1_loss(recon_x, x, reduction='sum')\n",
    "    elif loss_type == 'bce':\n",
    "        recon_error = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (recon_error + kl) / x.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/null/code.png\" style=\"height:50px;display:inline\"> Task 1 - $\\beta$-VAE\n",
    "---\n",
    "In the standard VAE, an isotropic Gaussian ($p(z) \\sim \\mathcal{N}(0, I)$) is typically assumed as the prior distribution for z. Note that under this distribution the components of z are independent (e.g. disentangled - a disentangled representation can be defined as one where single latent units are sensitive to changes in single generative factors, while being relatively invariant to changes in other factors) which is exactly the property we would like our approximate posterior distribution (e.g. $q(z|x)$) to have. Thus, to encourage independence we increase the KL-divergence term in the ELBO by a factor of $\\beta$: $$ \\mathcal{L}_{\\beta-VAE} = -\\mathbb{E}_{q_{\\phi(z|x)}}[p_{\\theta}(x|z)] + \\beta \\cdot D_{KL}[q_{\\theta}(z|x) || p(z)]  $$\n",
    "   * Training is performed exactly the same as for the standard VAE.\n",
    "   * When $\\beta=1$, it is same as VAE.\n",
    "   * When $\\beta>1$, it applies a stronger constraint on the latent bottleneck and limits the representation capacity of $z$. \n",
    "       * For some conditionally independent generative factors, keeping them disentangled is the most efficient representation.\n",
    "   * When $0<\\beta<1$, it can be interpreted as optimizing an approximate log marginal likelihood bound under an alternative prior, regularized to prevent degeneracy (of the KL-divergence).\n",
    "   \n",
    "The tasks:\n",
    "1. Modify the loss function to support $\\beta$-VAE. The function should return the reconstruction loss, the kl-divergence (**without the multiplication by $\\beta$**) and the total loss.\n",
    "    * The signature of the function should be: `beta_loss_function(recon_x, x, mu, logvar, loss_type='bce', beta=1)`\n",
    "    * The reconstruction loss, the kl-divergence and and the total loss should be normalzied by the batch size.\n",
    "    * The returned reconstruction loss and kl-divergence should be converted to numpy: `kl_d.data.cpu().numpy()` (but only them, not the total loss)\n",
    "2. Load the MNIST dataset, as in the tutorial, and create a train loader.\n",
    "3. For $\\beta=[0.05, 0.5, 1, 5]$, train a $\\beta$-VAE for 50 epochs and keep track of:\n",
    "    * The average reconstruction loss in each epoch\n",
    "    * The average KL-divergence in each epoch\n",
    "    * A checkpoint of the network in the format: `beta_(value of beta)_vae_50_epochs.pth` (there is an example in the tutorial)\n",
    "        * DO NOT SUBMIT THE CHECKPOINTS, THEY ARE FOR YOU TO USE.\n",
    "    * This may take a while, so go grab a coffee in the meantime :)\n",
    "4. For $\\beta=[0.05, 0.5, 1, 5]$, plot the KL-divergence and reconstruction error vs. epochs.\n",
    "5. For $\\beta=[0.05, 0.5, 1, 5]$, generate 5 samples from the VAE and plot them (`imshow`...). Run this a couple of times to get an impression of the samples for each $\\beta$.\n",
    "\n",
    "\n",
    "* Notes:\n",
    "    * Be organized - separate to different code cells if it keeps you organized.\n",
    "    * Make sure to properly define the hyper-parameters (see tutorial), and define the `device` automatically. Don't forget to send all the models and tensors to the device. We will run your code on a GPU.\n",
    "    * If you are not satisfied with the results, and you have time, you can try and increase the number of epochs to 100, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 1 - $\\beta$-VAE\n",
    "---\n",
    "We will now analyze the results. Answer the following questions:\n",
    "1. Explain intuitively the loss function of $\\beta$-VAE. In your answer, explain the trade-off between the reconstruction loss and the KL-divergence and how it is affected by the $\\beta$ parameter. Hint: think about regularization as you learned in the ML course (for example, in linear regression).\n",
    "2. What is the main trend in the KL and reconstruction loss vs. epochs? In your answer, you should compare between the $\\beta$'s.\n",
    "3. For what values of $\\beta$ you would expect better reconstruction (why would we want better reconstruction?) and for what values you would expect higher-quality samples? In your answer, refer to the blurriness in the samples you plotted.\n",
    "4. Run the cell where you plot the samples (if you separated the cells for each $\\beta$, then run all of them) a couple of times (just hit Ctrl + Enter). For which value of $\\beta$ there is more *diversity* in the samples? (i.e., if out of 6 samples you get four 9's, it is not actually diverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/null/code.png\" style=\"height:50px;display:inline\"> Task 2 - Conditional VAE (CVAE)\n",
    "---\n",
    "Conditional Variational Autoencoder (CVAE) is an extension of Variational Autoencoder (VAE).\n",
    "However, as you may have noticed, we have no control on the data generation process of the VAE. That is, for example, on MNIST, we could not control the latent space, and when we sampled, we would not know what digit would be generated. This could be problematic if we want to generate some specific data. As an example, suppose we want to convert a unicode character to handwriting. In vanilla VAE, there is no way to generate the handwriting based on the character that the user inputted. Concretely, suppose the user inputted character ‘2’, how do we generate handwriting image that is a character ‘2’? We couldn’t.\n",
    "\n",
    "Hence, CVAE was developed. Whereas VAE essentially models latent variables and data directly, CVAE models latent variables and data, both conditioned to some random variables.\n",
    "Recall, on VAE, the objective is:\n",
    "$$\\log P(X) -  D_{KL}[Q(z|X) || P(z|X)]  =  \\mathbb{E}_{Q(z|X)}[\\log P(X|z)] -D_{KL}[Q(z|X)|| P(z)]  $$\n",
    "that is, we want to optimize the log likelihood of our data $P(X)$ under some “encoding” error. The original VAE model has two parts: the encoder $Q(x|Z)$ and the decoder $P(X|z)$.\n",
    "\n",
    "If we focus on the encoder, it models the latent variable $z$ directly based on $X$, and it doesn't care about the different types of $X$ (e.g., it doesn't care if it is 1 or an 8). But wait, this was all the idea of **unsupervised learning**, we have no access to labels. Well, in this case, we have some information (thus, CVAE is sometimes referred to as **semi-supervised** learning model).\n",
    "\n",
    "Similarly, in the decoder part, it only models $X$ directly based on the latent variable $z$. So, how do we tell the VAE what we want to generate? We can condition encoder and decoder to another things, let's denote them with $c$ (for \"condition\").\n",
    "\n",
    "* The encoder is now conditioned on 2 variables- $X, c$: $Q(z|X,c)$ and the decoder in now conditioned on- $z$, $c$: $P(X|z,c)$\n",
    "* Hence, our variational lower bound objective is now in this following form: $$ \\log P(X|c) -  D_{KL}[Q(z|X,c) || P(z|X,c)]  =  \\mathbb{E}_{Q(z|X,c)}[\\log P(X|z,c)] -D_{KL}[Q(z|X,c)|| P(z|c)] $$ (we just conditioned all of the distributions with a variable $c$)\n",
    "* So what is different? Almost nothing! We still model $P(z|c) \\sim \\mathcal{N}(0,I)$, and the rest are modeled by the neural network.\n",
    "* But how is it done in practice? Simple! **Concatenation**: instead of encoding $X$, we encoded $[X,c]$, that is, we concatenate them. Same for the decoder: we take the latent variable $z$ and concatenate with $c$ and then the input of the decoder is $[z,c]$.\n",
    "* In PyTorch, we concatenate with `x = torch.cat([x, x_cond], dim=1)` (the 0 dimension is the batch dimension).\n",
    "\n",
    "The tasks:\n",
    "* Load the Fashion-MNIST dataset, as in the tuorial, and create a train loader. Note that you get both the images and their **labels**.\n",
    "* The labels are the classes (0-9). In order to use them in the network we need to convert them to one-hot vectors (0 -> [1,0,0,0,0,0,0,0,0,0], 1 -> [0,1,0,0,0,0,0,0,0,0] ...). The length of the one hot vector in as the number of classes. You are given a function that converts ints to one-hot vectors, use it on the labels, before you perform the concatenation.\n",
    "* Modify the VAE architecture to support conditionals.\n",
    "    * Copy-paste the skeleton (the original VAE, from the begining of the tutorial), and just modify the current functions. Note that there **are very few** changes needed.\n",
    "    * Here are some tips, but feel free to implement as you wish, as long as it works:\n",
    "        * In Python, you can let a function input be `None`, and then if the user inputs something that is not `None`, the function would act different. Here is an example: `def encode(x, x_cond=None): if x_cond is not None: ...`\n",
    "        * Here are the parts that we recommend you change:\n",
    "            * In the Vae module:\n",
    "                * `def __init__(self, x_dim=28*28, z_dim=10, hidden_size=256, device=torch.device(\"cpu\"), cond_dim=None)`\n",
    "                * `def sample(self, num_samples=1, x_cond=None)`\n",
    "                * `def forward(self, x, x_cond=None)`\n",
    "            * Note that these are the minimal changes that can be done to implement VAE that supports CVAE. No need to modify VaeEncoder and VaeDecoder when we are using fully-connected layers. However, if we were to use convolutional layers, we would have to change also the encoder and decoder since convolutional layers work on images, and only after the images features from the convolutional layers have been extracted, we would concatenate the the condional $c$ (just before the fully-connected layers that output $\\mu, \\sigma$.\n",
    "* Train CVAE on the Fashion-MNIST dataset (100 epochs, at least). Use $\\beta$-VAE loss function (it shouldn't have changed from the regular VAE. Save a checkpoint of the network in the format: `fmnist_beta_(value of beta)_cvae_(number of epochs)_epochs.pth`. The rest of the hyper-parameters are up to you.\n",
    "* Plot $n_{samples}=6$ from the CVAE for 6 classes of your choosing.\n",
    "* **Tip**: this may take a while, so if everything seems to work, let it run on Google Colab and go grab another coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hots(batch, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts batch of integes numbers to one-hot vector given the vector length\n",
    "    :param batch: batch of values to convert\n",
    "    :param num_classes: length of the vector\n",
    "    :return: one_hot_batch\n",
    "    \"\"\"\n",
    "    one_hot_batch = torch.zeros(batch.size(0), num_classes).to(batch.device)\n",
    "    for i in range(batch.size(0)):\n",
    "        one_hot_batch[i, int(batch[i].data.cpu().item())] = 1\n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/null/ask-question.png\" style=\"height:50px;display:inline\"> Question 2 - Conditional VAE\n",
    "---\n",
    "1. Can we perform interpolation of the latent space as we did in the tutorial? What is the meaning of doing **in-class** interpolation in the case of Fashion-MNIST? Explain.\n",
    "2. Why did we convert the the classes number to one-hot vectors? Think of the other inputs to the networks and the values that the neurons accept. What is the risk of using just one number as the condition instead of a vector?\n",
    "3. How is the quality of the samples? How can the quality be improved? In you answer, refer to the bluriness in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/null/code.png\" style=\"height:50px;display:inline\"> Task 2 - BONUS - Pokemon: Gotta Generate 'Em All!\n",
    "---\n",
    "This is a **non-mandatory**, more challenging task. Come back here only if you are done with the rest of the exercise and want to take on a challenge.\n",
    "\n",
    "* Note - you should pick one of the following:\n",
    "    * Fully-connected - work with grayscale images (5 points)\n",
    "    * Convolutional - work with RGB images (10 points)\n",
    "* No matter what type of model you chose, you need to answer the bonus questions that follow the code.\n",
    "\n",
    "In this task we are going to (try) generate new pokemons! Our dataset includes ~900 pokemons. Each sample is a 60x60 image and the type of the pokemon (18 classes). The type is already in one-hot form. If you have access to a GPU, we recommend trying the convolutional version of this task.\n",
    "\n",
    "Let's look at the data, for the **fully-connected** version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon_dataset import PokemonDataset\n",
    "poke_data = PokemonDataset(root='./data/pokemon', rgb=False)\n",
    "sample_dataloader = DataLoader(poke_data, batch_size=6, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10 ,5))\n",
    "samples, labels = next(iter(sample_dataloader))\n",
    "for i in range(samples.size(0)):\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.imshow(samples[i][0].data.cpu().numpy(), cmap=\"gray\")\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "print(\"can you name these pokemons?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the types encoding\n",
    "poke_data.type_to_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**: Implement CVAE for the pokemon dataset, it should not be different than the one you implemented for Fashion-MNIST. The task is to define the model and tune the hyper-parameters. \n",
    "Note that due to being a really small dataset (only 900 examples!) you will need a really large number of epochs to get something. We don't expect to see actual pokemons, but we want to see the that the networks learned something. You will also need a larger latent space dimension, as pokemons are more complex than cloths.\n",
    "\n",
    "* If you have taken the **Computer Vision** course - you should add **data augementations** to the images, to create a more diverse dataset. Recommended augmentations: `RandomHorizontalFlip`, `ColorJitter`, random horizontal translation (up to 5 pixels).\n",
    "\n",
    "\n",
    "\n",
    "* Train CVAE on the pokemon dataset. Save a checkpoint once you are done: `pokemon_beta_(value of beta)_vae_(num epochs)_epochs.pth`.\n",
    "* Plot samples for at least 6 types of your choosing (try to find the better ones).\n",
    "* Plot reconstructions for at least 6 types of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have access to a GPU and feel adventurous (CNN version)?\n",
    "* If you have completed the fully-connected verion, you can just skip to the bonus questions.\n",
    "\n",
    "If you feel creative and want to work with CNNs, we are giving you the VaeCnnEncoder and VaeCnnDecoder architectures, and all you have to do is implement the Vae class using these, and run the VAE with RGB images.\n",
    "Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokemon_dataset import PokemonDataset\n",
    "poke_data = PokemonDataset(root='./data/pokemon', rgb=True)\n",
    "sample_dataloader = DataLoader(poke_data, batch_size=6, shuffle=True, drop_last=True)\n",
    "\n",
    "fig = plt.figure(figsize=(10 ,5))\n",
    "samples, labels = next(iter(sample_dataloader))\n",
    "for i in range(samples.size(0)):\n",
    "    ax = fig.add_subplot(2, 3, i + 1)\n",
    "    ax.imshow(samples[i].permute(1, 2, 0).data.cpu().numpy())  # permute to (Height, Width, Channels)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "print(\"can you name these pokemons?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "* Impelement the convolutional CVAE and train it. Save a checkpoint `\"pokemon_cnn_beta_(value of beta)_vae_(num epochs)_epochs.pth\"`\n",
    "* Plot samples for at least 6 types of your choosing (try to find the better ones).\n",
    "* Plot reconstructions for at least 6 types of your choosing.\n",
    "\n",
    "\n",
    "* Note that the call to the loss function in the training loop is in the form: `loss = loss_function(x_recon, x.permute(0, 2, 3, 1), mu, logvar, loss_type='bce', beta=beta)`.\n",
    "    * This is because `x_recon` is in the shape (batch_size, H, W, C) and `x` is (batch_size, C, H, W).\n",
    "* Use a lower leraning rate (start with `1e-4`).\n",
    "\n",
    "* Components:\n",
    "    * Conv2D - `nn.Conv2d(in_channels, out_channels, kernel_size, stride)`\n",
    "    * Deconv2d - `nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)`\n",
    "    * Batch Normalization 1D - `nn.BatchNorm1d(num_features, affine=True)`\n",
    "    * Batch Normalization 2D - `nn.BatchNorm2d(num_features, affine=True)`\n",
    "    * Calculate the convolutional output size with `_get_conv_out(self, shape)` as in tutorial 8.\n",
    "    * FC/Linear - `nn.Linear(in, out)`\n",
    "\n",
    "#### Encoder Architecture - `VaeCnnEncoder(torch.nn.Module)`\n",
    "* Block 1:\n",
    "    * Conv2d - `in_channels=3, out_channels=128, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - 128 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 2:\n",
    "    * Conv2d - `in_channels=128, out_channels=64*4, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - $64*4$ features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 3:\n",
    "    * Conv2d - `in_channels=64*4, out_channels=64*8, kernel_size=(3, 3), stride=(2, 2), padding=1`\n",
    "    * Batch Normalization 2D - $64*8$ features\n",
    "    * ReLU (or any other activation you want)\n",
    "* FC1 ($\\mu$) - `nn.Linear(self._get_conv_out(x_shape) + self.cond_dim, self.z_dim)`\n",
    "* FC1 ($\\Sigma$) - `nn.Linear(self._get_conv_out(x_shape) + self.cond_dim, self.z_dim)`\n",
    "\n",
    "#### Decoder Architecture - `VaeCnnDecoder(torch.nn.Module)`\n",
    "* FC1 ($z$) - `nn.Linear(self.z_dim (+cond_dim), 64 * 4 * 4 * 4)`\n",
    "* Batch Normalization 1D - $64*4*4*4$ features.\n",
    "* Block 1:\n",
    "    * Deconv2d - `in_channels=64 * 4, 64 * 2, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "    * Batch Normalization 2D - 128 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 2:\n",
    "    * Deconv2d - `in_channels=128, 64, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "    * Batch Normalization 2D - 64 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Block 3:\n",
    "    * Deconv2d - `in_channels=64, 64, kernel_size=(3, 3), stride=(2, 2), padding=2, output_padding=1`\n",
    "    * Batch Normalization 2D - 64 features\n",
    "    * ReLU (or any other activation you want)\n",
    "* Deconv2d - `in_channels=64, 3, kernel_size=(3, 3), stride=(2, 2), padding=1, output_padding=1`\n",
    "* Sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Bonus Question\n",
    "---\n",
    "1. What do you think about the results on the Pokemon dataset? Name at least 2 reasons for the VAE somewhat low performance on the Pokemon dataest.\n",
    "2. Suggest ideas to improve the performance (at least 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/null/grand-master-key.png\" style=\"height:50px;display:inline\"> Part 2 - Clustering\n",
    "---\n",
    "In this task, we will use a graph convolutional network (GCN) to perform semi-supervised clustering.\n",
    "Our goal is to use the graph structure to get accurate clusters for the data.\n",
    "\n",
    "We will make use of the Pytorch geometric (PyG) library to provide us with implementations of GCNs and for importing datasets.\n",
    "See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for installation instructions for your platform.\n",
    "\n",
    "You may implement graph convolution layers by yourself if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the exrcise - part 2\n",
    "# you can add more if you wish (but it is not really needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "# PyG imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets.karate import KarateClub\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "warnings.filterwarnings(category=UserWarning, action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/null/code.png\" style=\"height:50px;display:inline\"> Task 3 - Implementing a GCN\n",
    "---\n",
    "* Implement a GCN module that has two graph convolution layers and a non-linearity.\n",
    "    * Layer 1 should map the input features to a hidden representation\n",
    "    * A ReLU activation should be run on the output of this first Graph convolution layer\n",
    "    * Layer 2 should map the hidden representation to the outputs\n",
    "    * You can use `GCNConv` from PyG for implementations of graph convolution layers, but do not use any models from PyG.\n",
    "* Write your code in the blocks below such that the last block runs. \n",
    "    * The line `GCN(n_node_features, hidden_layer_size, num_clusters)` should run the initialization of the GCN.\n",
    "    * The forward implementation should work with pyG data that has fields `data.x, data.edge_index` with signature `forward(self, data)`\n",
    "* Print the resulting accuracy from running the test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karate_dataset = KarateClub()\n",
    "n_node_features = 34 #  from documentation\n",
    "num_clusters = 4 #  from documentation\n",
    "hidden_layer_size = 16 #  chosen by us\n",
    "n_epochs = 20 #  chosen by us\n",
    "\n",
    "gcn_model = GCN(n_node_features, hidden_layer_size, num_clusters)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = gcn_model.to(device)\n",
    "data = karate_dataset.data.to(device)\n",
    "print(data.x)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "train_mask = data.train_mask \n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Classification accuracy for the graph data\n",
    "model.eval()\n",
    "test_mask = ~ train_mask\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[test_mask] == data.y[test_mask]).sum()\n",
    "acc = int(correct) / int(test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 3 - GCN basics\n",
    "---\n",
    "1. What are the features in the Karate Club dataset? How does this feature definition effect the GCN activation $AXW^{(1)}$?\n",
    "2. For this GCN, how many neighbors is each node affected by? How did you determine this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/null/code.png\" style=\"height:50px;display:inline\"> Task 4 - Clustering with a GCN\n",
    "---\n",
    "Now that we have trained a GCN with one labelled sample per class, we have a GCN that creates a good mapping of new points to clusters.\n",
    "We note that combining GCNs and autoencoders can give us unsupervised clustering and seem to work well, but we will focus on the simpler setting where we have a few labels per class/cluster (in this case, one example).\n",
    "\n",
    "1. Create a new GCN with `hidden_layer_size=4` and output dimension 2\n",
    "2. Visualize the output of the untrained network as a two-dimensional embedding. Show different classes in different colors.\n",
    "3. Train this GCN on the training data (use `data.train_mask`) of one example per class\n",
    "    * Create a simple 2D embedding for class labels\n",
    "        * for example, by mapping `labels = torch.vstack([2*(data.y % 2)-1, 2*((data.y>1).float())-1]).T`\n",
    "    * Use MSE loss between the network output and the label embedding\n",
    "    * You can use a `tanh` activation function instead of ReLU to get better results\n",
    "    * Use the same optimizer as above: `torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)`\n",
    "    * Run 300 training epochs\n",
    "4. Visualize the output of the trained network on the entire dataset with the different classes in different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - you can use as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/48/000000/ask-question.png\" style=\"height:50px;display:inline\"> Question 4 - GCN for embedding\n",
    "---\n",
    "1. What is the effect of the semi-supervised training on the overall embedding? refer to the plots in your answer.\n",
    "2. How does it make sense that training on only one point per class is enough to give a clustered embedding that separates classes well? Refer to the specific structure of GCN layers. Consider the differences between the embedding before and after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
