{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2eT7lmoM0e5K+Kgjua4vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrobotov/HW/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Song classification using Variational Auto-Encoder**\n",
        "\n",
        "\n",
        "# First note\n",
        "\n",
        "first thing, copy the following files from the sample data folder up:\n",
        "DataSet.py\n",
        "variational_auto_encoder.py\n",
        "\n",
        "dont run this before you read the instructions on the bottom\n",
        "\n"
      ],
      "metadata": {
        "id": "nAb_lClRPqHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "starting with imports:"
      ],
      "metadata": {
        "id": "37k6WRWkkZxf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kyzbHw7KPWAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a140b1-7a74-486c-cfaf-7606acd2cc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python_speech_features in /usr/local/lib/python3.8/dist-packages (0.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install python_speech_features\n",
        "from DataSet import DataSet\n",
        "from scipy.io import wavfile\n",
        "import python_speech_features as psf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from variational_auto_encoder import Vae, beta_loss_function\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import time\n",
        "transform = torchvision.transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this piece of code defines the function we will use to perform the window in the MFCC,\n",
        "the function we use to read the files and the mfcc transform\n"
      ],
      "metadata": {
        "id": "IS8M0TxjPhyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_winfunc = lambda x: np.blackman(x)  # np.hamming(x) np.kaiser(M=x, beta=14)# # 'Hamming' # [ frame window type ]\n",
        "def file_read(filename):\n",
        "    fs,data = wavfile.read(filename)\n",
        "    return fs,data\n",
        "\n",
        "\n",
        "def extract_features(data,params):\n",
        "    X_mfcc = psf.mfcc(  # The audio signal from which to compute mfcc_features.\n",
        "        data.transpose(),\n",
        "        # The samplerate of the signal we are working with.\n",
        "        params.fs,\n",
        "        # The length of the analysis window in seconds.\n",
        "        # Default is 0.025s (25 milliseconds)\n",
        "        winlen=params.mfcc_winsize,\n",
        "        # The step between successive windows in seconds.\n",
        "        # Default is 0.01s (10 milliseconds)\n",
        "        winstep=params.mfcc_winstep,\n",
        "        # The number of cepstrum to return.\n",
        "        # Default 13.\n",
        "        numcep=params.mfcc_num_coeffs,\n",
        "        # The number of filters in the filterbank.\n",
        "        # Default is 26.\n",
        "        nfilt=params.mfcc_filters,\n",
        "        # The FFT size. Default is 512.\n",
        "        nfft=params.mfcc_nfft,\n",
        "        # If true, the zeroth cepstral coefficient is replaced\n",
        "        # with the log of the total frame energy.\n",
        "        appendEnergy=True,\n",
        "        # use this window for framing\n",
        "        winfunc=mfcc_winfunc)\n",
        "    return np.square(np.abs(np.fft.fft(X_mfcc)))"
      ],
      "metadata": {
        "id": "00mWpffdVlLQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here we do the file read\n",
        "NOTICE - the name of the directory here is: 'debug set'\n",
        "you need to change it when you load your own dir\n",
        "\n",
        "this directory should contain dub directories with the genre/class name and the songs inside each sub dir\n",
        "\n",
        "(length doesn't matteer right now)"
      ],
      "metadata": {
        "id": "ncYucURxYjlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_name='debug set'\n",
        "data_set = DataSet(name=set_name,debug_mode=True,file_read=file_read)\n",
        "data_set.read()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "QAQHXRgYY1S-",
        "outputId": "a228d486-0397-4f12-bf44-749b980f3d7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set - Initialized Set debug set\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f5313d3f90a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'debug set'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_read\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/DataSet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DataSet.py\u001b[0m in \u001b[0;36mread_files\u001b[0;34m(self, max_num_samples)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_num_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'debug set'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and transforming the files to a matrix (or rather a tensor, since we are using pytorch):\n",
        "\n",
        "starting with the case that we want to perform MFCC:\n"
      ],
      "metadata": {
        "id": "Gn2pCBVjZC6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class parameters:\n",
        "    def __init__(self,mfcc_winsize=0.02,mfcc_winstep=0.01,mfcc_num_coeffs=13,mfcc_filters=26,mfcc_nfft=int(32768 / 32),kmeans_cluster=15):\n",
        "        self.mfcc_winsize =  mfcc_winsize # [seconds] - affects M dimension of feature vector\n",
        "        self.mfcc_winstep = mfcc_winstep  # [seconds] - affects feature vector smoothness\n",
        "        self.mfcc_num_coeffs = mfcc_num_coeffs  # [number of Mel Frequency Cepstrum Coefficient] feature vector N dimension\n",
        "        self.mfcc_filters = mfcc_filters # [number of MFCC filters] has to be larger or equal to NUM_MFCC\n",
        "        self.mfcc_nfft = mfcc_nfft  # [ number of frames for fft]\n",
        "        self.kmeans_cluster = kmeans_cluster  # [ number of kmeans cluster for features ]\n",
        "        self.mfcc_winfunc = mfcc_winfunc\n",
        "        self.fs = data_set.fs\n",
        "        \n",
        "for x_vec,y_class in zip(data_set.X,data_set.y):\n",
        "\n",
        "    X_mfcc = extract_features(x_vec,params)\n",
        "    if len(X) == 0:\n",
        "        X = X_mfcc\n",
        "        y = np.repeat(y_class,np.size(X_mfcc,0))\n",
        "    else:\n",
        "        X = np.vstack((X, X_mfcc))\n",
        "        y = np.append(y,np.repeat(y_class,np.size(X_mfcc,0)))"
      ],
      "metadata": {
        "id": "DAkZgg9XZjQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "X = data_set.X\n",
        "y = data_set.y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bbNHvX6KZQRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **OK now we start with the fun**\n",
        "\n",
        "the following params are:\n",
        "\n",
        "FRAME_SIZE - number of dimensions per sample, in case we take the raw audio this should be Fs* [Number of seconds] (44100*N)\n",
        "\n",
        "Z_DIM - the number of dimensions the algorithm will try to represent the input \n",
        "\n",
        "> data (in the example of the digits it was Z = 10 )\n",
        "\n",
        "BETA - this is related to this specific version of the algorithm \n",
        "this beta scales between two errors, in the form of:\n",
        "\n",
        "> E_total = E_1 + beta*E_2\n",
        "> the range is about ~ 0.05 - 10\n",
        "> when 1 is considered the normal \"Vanilla\" version\n",
        "\n",
        "NUM_EPOCHS - an epoch is a full pass over the training set.\n",
        "> when we train we want to pass a lots of times on the set\n",
        "\n",
        "LEARNING_RATE - this impacts the learning step size.\n",
        "> if the error from epoch to epoch is overshooting, we should conider reducing the leanrning rate\n",
        "\n",
        "HIDDEN_SIZE - this parameter is related to the algorithm itself and it contrpls the size of the intermidaite level of neurons\n",
        "\n",
        "\n",
        "following the parameters is scaling of X\n",
        "and some trial of capturing a pytorch remote machine the has a gpu\n",
        "\n",
        "after that some required inits:"
      ],
      "metadata": {
        "id": "P310b0mSVw8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_SIZE = int(44100/60)\n",
        "Z_DIM = 10 # size of the latent dimension\n",
        "BETA =  0.05 #[0.05,0.5,1,5]\n",
        "LEARNING_RATE = 1e-4 # for the gradient optimizer\n",
        "NUM_EPOCHS = 100 #\n",
        "HIDDEN_SIZE =  1024 # size of the hidden layers in the networks\n",
        "\n",
        "\n",
        "X = X.reshape(-1,X_DIM)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# check if there is gpu avilable, if there is, use it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.current_device()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "print(\"running calculations on: \", device)\n",
        "# create our model and send it to the device (cpu/gpu)\n",
        "vae = Vae(x_dim=X_DIM, z_dim=Z_DIM, hidden_size=HIDDEN_SIZE, device=device).to(device)\n",
        "# optimizer\n",
        "vae_optim = torch.optim.Adam(params=vae.parameters(), lr=LEARNING_RATE)\n",
        "x = transform(X_scaled)#.to(device).view(-1, X_DIM)  # just the images\n",
        "train_losses = []\n",
        "recon_mean = []\n",
        "kl_mean = []\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dwnRvs9_XSC4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## here the learning begins:\n"
      ],
      "metadata": {
        "id": "EV-qFvK4chJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    batch_losses = []\n",
        "    recon_loss = []\n",
        "    kl_loss = []\n",
        "\n",
        "    x_recon, mu, logvar, z = vae(x.to(device=device, dtype=torch.float32))\n",
        "    # calculate the loss\n",
        "    recon, kl, loss = beta_loss_function(x_recon, x.to(device=device, dtype=torch.float32), mu, logvar, loss_type='bce', beta=BETA)\n",
        "    # optimization (same 3 steps everytime)\n",
        "    vae_optim.zero_grad()\n",
        "    loss.backward()\n",
        "    vae_optim.step()\n",
        "    # save loss\n",
        "    batch_losses.append(loss.data.cpu().item())\n",
        "    recon_loss.append(recon)\n",
        "    kl_loss.append(kl)\n",
        "\n",
        "    train_losses.append(np.mean(batch_losses))\n",
        "    if epoch > 0:\n",
        "        print(\"epoch: {} training loss: {:.3f} stride: {:.2f} epoch time: {:.2f} sec\".format(epoch, train_losses[-1], train_losses[-2]-train_losses[-1],\n",
        "                                                                          time.time() - epoch_start_time))\n",
        "    recon_mean.append(np.array(recon_loss).mean())\n",
        "    kl_mean.append(np.array(kl_loss).mean())\n",
        "    # save\n",
        "fname = \"beta_(\" + str(BETA) + \" )_vae.pth\"\n",
        "torch.save(vae.state_dict(), fname)\n",
        "print(\"saved checkpoint @\", fname)"
      ],
      "metadata": {
        "id": "_KxnN7pLcgyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at the end it saves the vae dict to a file which can be loaded with:"
      ],
      "metadata": {
        "id": "NrUmdVrqfzYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "vae=Vae(x_dim=X_DIM,z_dim=Z_DIM,hidden_size=HIDDEN_SIZE,device=device).to(device)vae.load_state_dict(torch.load(fname))\n",
        "print(\"loaded checkpoint from\",fname)"
      ],
      "metadata": {
        "id": "UAuiN7d4gLRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "(1) connect this code with as large as possible song data base\n",
        "\n",
        "(2) go through tthe notebook, and run section by section\n",
        "\n",
        "(3) create a loop that runs on different variations of params, somting like this:"
      ],
      "metadata": {
        "id": "H0HnA6qvgfUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BETA =  [0.05,0.5,1,5]\n",
        "for beta in BETA:\n",
        "  # do stuff"
      ],
      "metadata": {
        "id": "ZzwIibA6hRis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(4) write a function that converts the matrix back to an audio file\n",
        "> test it with the original X\n",
        "\n",
        "(5) try to find the params that best reconstruct the audio\n",
        "\n",
        "key notes:\n",
        "> enlarging the Z dim can improve results due to richer representation\n",
        "> learning_rate might be changeable from epoch to epoch - explore it\n",
        "> the larger the frame size the chalenging it is to forecast it\n",
        "> larger hidden layer size larger pattern recognition ability - but beware of overfitting - that is when we learn the particular data set \"too\" good\n",
        "\n",
        "good luck!!  "
      ],
      "metadata": {
        "id": "FQY1crV-hhEP"
      }
    }
  ]
}